#!/opt/common/CentOS_6-dev/python/python-2.7.10/bin/python
from cmo import workflow
import argparse, os, sys
import cmo 
#WHOA, existentially troubling, man
PYTHON = cmo.util.programs['python']['default']

def construct_workflow(tumorbam, normalbam, tag, facets_args, output_dir,snps=None, tumor_sample=None, normal_sample=None):
    #look at @RG SM: tag for samples
    if not tumor_sample: 
        tumor_sample = cmo.util.infer_sample_from_bam(tumorbam)
    if not normal_sample:
        normal_sample = cmo.util.infer_sample_from_bam(normalbam)
    if normal_sample == tumor_sample:
        print >>sys.stderr, "Sample names in normal and tumor are the same- forcibly override one or both to use this pipeline"
        sys.exit(1)
    if not tag and tumor_sample and normal_sample:  
        tag = tumor_sample + "__" + normal_sample
    elif not tumor_sample:
        print >>sys.stderr, "Can't infer tumor sample name from BAM file-- please supply it to workflow"
        sys.exit(1)
    elif not normal_sample:
        print >>sys.stderr, "Can't infer normal sample name from BAM file-- please supply it to workflow"
        sys.exit(1)
    default_basecount_options = [ "--sort_output", "--compress_output", "--filter_improper_pair 0"]
    if not output_dir:
        #TODO make directory safe for invalid dir chars in sample names
        output_dir = os.path.join(os.getcwd(), tag, '')
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    #if the idiot user supplied relative path we must fix
    output_dir = os.path.abspath(output_dir)
    
    #count jobs
    count_jobs = []
    tumor_normal_counts = []
    
    for (bam, base) in [(tumorbam, tumor_sample), (normalbam, normal_sample)]:
        out_file = os.path.abspath(os.path.join(output_dir, base + ".dat"))
        if os.path.exists(out_file + ".gz"):
            #pretend we did this shitty slow step
            tumor_normal_counts.append(out_file)
            continue
        #first we add tumor, then normal - the order mergeTN expects them
        tumor_normal_counts.append(out_file)

        basecount_cmd = ["cmo_getbasecounts", "--bam", bam, "--out", out_file] + default_basecount_options
        if(snps):
            basecount_cmd = basecount_cmd + ["--vcf" ,os.path.abspath(snps)]
        #print " ".join(basecount_cmd)
        job = workflow.Job(" ".join(basecount_cmd), est_wait_time="59", resources="rusage[mem=40]", name="getBasecounts " + base)

        count_jobs.append(job)
   
    #merge job
    merged_counts = os.path.join(output_dir, "countsMerged____" + tag + ".dat.gz")
    merge_job= None
    if not os.path.exists(merged_counts):
        merge_cmd = ["cmo_facets mergeTN", "-t",  tumor_normal_counts[0], "-n", tumor_normal_counts[1], "-o", merged_counts]
        #print " ".join(merge_cmd)
        merge_job = workflow.Job(" ".join(merge_cmd), est_wait_time="59", resources="rusage[mem=60]", name="mergeTN " + tag)
  
    #facets job
    #args will be [--foo, value] or [-f, value] in this list
    facets_dir = "facets_"
    if not facets_args or len(facets_args) ==0:
        facets_args = []
        facets_dir += "default"
    else:
        it = iter(facets_args)
        for val in it:
            arg = val.lstrip("-")[0]
            value = next(it)
            facets_dir += "%s-%s" % (arg, value)
    facets_dir = os.path.join(output_dir, cmo.util.filesafe_string(facets_dir))
    if os.path.exists(facets_dir):
        print >>sys.stderr, "This facets setting directory already exists- bailing out - RM it to force rerun"
        sys.exit(1)
    else:
        print >>sys.stderr, "created facets subdir for these settings: %s" % facets_dir
        os.makedirs(facets_dir)
    facets_cmd = ["cmo_facets", "doFacets", "-f", merged_counts, "-t", tag, "-D", facets_dir] + facets_args
    facets_job = workflow.Job(" ".join(facets_cmd), est_wait_Time="59", name="Run Facets")
    dependencies = {}
  
  #FIXME: can we have a merge exist without the counts file?
    #if so this set of ifs needs to be redone
    jobs = []
    if len(count_jobs) > 0:
        dependencies[count_jobs[0]]=[merge_job]
        dependencies[count_jobs[1]]=[merge_job]
        jobs = jobs + count_jobs
    if(merge_job):
        dependencies[merge_job]=[facets_job]
        jobs.append(merge_job)
    #make workflow
    jobs.append(facets_job)
    return (jobs, dependencies, " ".join(["Facets", tag]), facets_job)



if __name__=='__main__':
    parser = argparse.ArgumentParser(description="Run Facets on luna!", epilog="Include any FACETS args directly on this command line and they will be passed through")
    parser.add_argument("--normal-bam", required=True, help="The normal bam file")
    parser.add_argument("--tumor-bam", required=True, help="The Tumor bam file")
    parser.add_argument("--tag", help="The optional tag with which to identify this pairing, default TUMOR_SAMPLE__NORMAL_SAMPLE")
    parser.add_argument("--vcf", help="override default FACETS snp positions")
    parser.add_argument("--output-dir", help="output dir, will default to $CWD/TAG_NAME/")
    parser.add_argument("--normal-name", help="Override this if you don't want to use the SM: tag on the @RG tags within the bam you supply-- required if your bam doesn't have well formatted @RG SM: tags")
    parser.add_argument("--tumor-name", help="Override this if you don't want to use the SM: tag on the @RG tags in the tumor bam you supply-- required if your bam doesnt have well formatted @RG SM: tags")
    parser.add_argument("--workflow-mode", choices=["serial","LSF"], default="LSF", help="select 'serial' to run all jobs on the launching box. select 'LSF' to parallelize jobs as much as possible on luna")
    (args, facets_args) = parser.parse_known_args()
    if args.output_dir:
        args.output_dir = os.path.abspath(args.output_dir)
    args.tumor_bam = os.path.abspath(args.tumor_bam)
    args.normal_bam = os.path.abspath(args.normal_bam)
    (jobs, dependencies, name, terminal_job) = construct_workflow(args.tumor_bam, args.normal_bam, args.tag, facets_args, args.output_dir, snps=args.vcf, tumor_sample = args.tumor_name, normal_sample=args.normal_name)
    facets_workflow = workflow.Workflow(jobs, dependencies, name=name)
    facets_workflow.run(args.workflow_mode)
         


